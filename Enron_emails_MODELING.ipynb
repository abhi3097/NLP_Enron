{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Deception in Enron Emails\n",
    "## 3. Modeling\n",
    "\n",
    "In this notebook, we attempt three methods of clustering and topic modeling, beginning with simple N-gram features, then reducing dimensionality for K-means using paragraph vectoring, or Doc2Vec. We also try variants of Latent Dirichlet Allocation (LDA) to identify sub-topics within emails for comparison to our keyword-targeted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Libraries and Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os, sys\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import email\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import sparse, hstack\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# run 'easy_install -U gensim' to install gensim and then re-launch jupyter notebook\n",
    "#import gensim\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import constants\n",
    "import utils\n",
    "import vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517401, 3)\n"
     ]
    }
   ],
   "source": [
    "# test import; runtime ~ 5 minutes for full set\n",
    "path = '/home/cmiller11/NLP_Enron'\n",
    "emails_df = pd.read_pickle(path + '/cleaned_emails.pkl')\n",
    "print(emails_df.shape)\n",
    "emails_df.head()\n",
    "\n",
    "# ENTER number of rows for training; NONE if using all\n",
    "num_rows = 50000\n",
    "\n",
    "if num_rows != None:\n",
    "    mini_df = emails_df.loc[range(0,num_rows),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Baseline: K-Means Clustering with Simple N-gram Features\n",
    "\n",
    "Using partition of early emails only to reduce dimensions in baseline bi-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams shape:  (50000, 168196)\n",
      "N-grams TF-IDF: (50000, 168196)\n"
     ]
    }
   ],
   "source": [
    "# consider longer n-grams\n",
    "transformer = TfidfTransformer()\n",
    "vectorize = CountVectorizer(ngram_range=(1, 2), max_df=.01, min_df=5)\n",
    "n_grams = vectorize.fit_transform(mini_df[\"email_str\"])\n",
    "print(\"N-grams shape: \", n_grams.shape)\n",
    "\n",
    "n_grams_idf = transformer.fit_transform(n_grams)\n",
    "print(\"N-grams TF-IDF:\", n_grams_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.01, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit k-means (n=4, tol=.01, max_iter=100 takes ~20 mins to train)\n",
    "kmeans = KMeans(n_clusters=5, tol=.01, max_iter=100)\n",
    "kmeans.fit(n_grams_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 clusters: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Positive labels:   [2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions on positively labeled examples\n",
    "base_preds = kmeans.predict(n_grams_idf)\n",
    "print(\"First 30 clusters:\", base_preds[:30])\n",
    "\n",
    "# GET IDS OF POSITIVE LABELS\n",
    "positive_labels = emails_df[\"suspicious_ind\"][emails_df[\"suspicious_ind\"]==1]\n",
    "\n",
    "print(\"Positive labels:  \", base_preds[positive_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 75864)\n"
     ]
    }
   ],
   "source": [
    "# evaluate closest examples in cluster 2 based on cosine similarity\n",
    "suspicious_ids = np.zeros(len(base_preds))\n",
    "for k in label_dict.keys():\n",
    "    suspicious_ids[k] = label_dict.get(k)\n",
    "\n",
    "# enter index of typical cluster\n",
    "key_cluster = 2\n",
    "\n",
    "cluster_ids = (base_preds==key_cluster).astype(int)\n",
    "\n",
    "features_np = sparse.csr_matrix.todense(feature_vects)\n",
    "\n",
    "labeled_feats = features_np[np.multiply(suspicious_ids, cluster_ids).astype(bool)]\n",
    "print(labeled_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([346, 27091, 373, 13966, 405])\n",
      "(5, 50000)\n",
      "[[21177 31738 39744 13961 11650 15532  7014   966   346  2759]\n",
      " [  985  2779  1893  2769   356   976  1879   373  2787   995]\n",
      " [  357   358  1892  2771   978   977  2820  1855   405  1027]\n",
      " [17248 10977 14371 11429 16116 13982 11606 13966  9998 17433]\n",
      " [20382 19225 29563 21933 27092 30259 24055 31124 27091 20569]]\n",
      "0.164362146998\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity\n",
    "print(label_dict.keys())\n",
    "cos_sims = cosine_similarity(labeled_feats, feature_vects)\n",
    "print(cos_sims.shape)\n",
    "closest = np.argsort(cos_sims, axis = 1)\n",
    "print(closest[:,-10:])\n",
    "print(cos_sims[1,985])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> jacques , the agreement looks fine . </s> <s> my only comment is that george and larry might object to the language that `` the bank that was requested to finance the construction of the project declined to make the loan based on the high costs of the construction of the project '' . </s> <s> <unk> , that bank lowered the loan amount based on lower estimates of <unk> which <unk> the amount of equity that would be required . </s> <s> did i loan them $ DGDGDGDGDGDGDG ? </s> <s> i thought it was less . </s> <s> regarding exhibit a , the assets include : the land , <unk> plans , engineering completed , appraisal , and <unk> study . </s> <s> most of these items are in a state of partial completion by the consultants . </s> <s> i have been speaking directly to the architect , engineer , and <unk> engineer . </s> <s> i am unclear on what is the best way to proceed with these consultants . </s> <s> the obligations should include the fees owed to the consultants above . </s> <s> do we need to list balances due or just list the work completed as an asset and give consideration of $ DGDGDGDG for the cash paid to the engineer and <unk> . </s> <s> phillip </s>\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nearest emails flagged:\n",
    "emails_df.loc[985, \"email_str\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 K-Means Clustering using Paragraph Vectors (doc2vec)\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['here', 'is', 'our', 'forecast', '<s>'], [0])\n"
     ]
    }
   ],
   "source": [
    "# create tagged documents for model training\n",
    "tagger = gensim.models.doc2vec.TaggedDocument\n",
    "tagged_docs = []\n",
    "for i, email in enumerate(emails_df[\"email_list\"]):\n",
    "    tagged_docs.append(tagger(email, [i]))\n",
    "\n",
    "print(tagged_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.5-win-amd64.egg\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "doc_model = gensim.models.doc2vec.Doc2Vec(documents=tagged_docs, vector_size = 1000, window = 5, alpha = .01, min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40709, 0.6937914490699768), (40678, 0.6881959438323975), (40849, 0.6868060231208801), (44116, 0.684868574142456), (7543, 0.6820092797279358), (44276, 0.680620014667511), (42515, 0.6795379519462585), (42274, 0.6784195899963379), (40589, 0.6751448512077332), (42558, 0.673206090927124)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_model.docvecs.most_similar(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec help:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "https://stackoverflow.com/questions/41709318/what-is-gensims-docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_feats = doc_model.docvecs.doctag_syn0\n",
    "print(doc_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_doc = KMeans(n_clusters=5, tol=.01, max_iter=100)\n",
    "kmeans_doc.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "(3000, 1644)\n"
     ]
    }
   ],
   "source": [
    "#Implement LDA for topics of each document\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#hyperparameters\n",
    "random.seed(24)\n",
    "#n_samples = 2000\n",
    "n_features = 2000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "rand_num = 3000\n",
    "\n",
    "rand_ids = random.sample(range(emails_df.shape[0]), rand_num)\n",
    "email_samples = emails_df.loc[rand_ids,]\n",
    "#training_emails = all_emails.loc[all_ids,]\n",
    "\n",
    "train_content = list(email_samples['email_str'])\n",
    "#dev_content = list(email_samples['content_str'])\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "#                                   max_features=n_features,\n",
    "#                                   stop_words='english')\n",
    "\n",
    "#tfidf = tfidf_vectorizer.fit_transform(all_content)\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.6, min_df=0.01,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                ngram_range = (1,1))\n",
    "#t0 = time()\n",
    "#tf = tf_vectorizer.fit_transform(data_samples)\n",
    "#print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#print()\n",
    "\n",
    "tf_train = tf_vectorizer.fit_transform(train_content)\n",
    "tf_vocab = tf_vectorizer.vocabulary_\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print(tf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_features=2000...\n",
      "done in 438.130s.\n",
      "with 499 iterations\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: ect enron hou pm corp ees cc na subject enron_development enronxgate lon john scheduled et david mark sat communications london\n",
      "Topic #1: com mail enron message subject sent original dg pm dgdgdg net intended mailto aol email recipient cc fw fax doc\n",
      "Topic #2: power energy california enron said state market electricity company dgdgdg electric utilities price prices billion dow ferc new jones customers\n",
      "Topic #3: gas dg day price market deal daily capacity natural pipeline month contract dgdgdgdgdg storage jan point need prices contracts chris\n",
      "Topic #4: know thanks subject enron let cc like need agreement just kay think meeting attached sent time did pm want ll\n",
      "Topic #5: 20 enron vince houston 01 business management kaminski trading group risk services team global president ena markets new weather london\n",
      "Topic #6: information enron data dg contact report access company service services new following time financial trading provide agreement questions issues request\n",
      "Topic #7: dg dgdgdg dgdgdgdgdgdg dgdgdgdgdg pm center time option table dgdgdgdgdgdgdg deals number deal area date border kate contact term houston\n",
      "Topic #8: error database iso final schedule schedules hour operation dg scheduling start file closed portland log perform hourahead date messages txt\n",
      "Topic #9: com million image new dg way company said news size click free year time world companies internet right home round\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "#hyperparameters\n",
    "lda_iterations = 1000\n",
    "    \n",
    "    \n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_features=%d...\"\n",
    "      % (n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=lda_iterations,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0, evaluate_every = 100)\n",
    "t0 = time()\n",
    "lda.fit(tf_train)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"with {} iterations\".format(lda.n_iter_))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.n_batch_iter_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
