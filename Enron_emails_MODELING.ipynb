{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Deception in Enron Emails\n",
    "## 3. Modeling\n",
    "\n",
    "In this notebook, we attempt three methods of clustering and topic modeling, beginning with simple N-gram features, then reducing dimensionality for K-means using paragraph vectoring, or Doc2Vec. We also try variants of Latent Dirichlet Allocation (LDA) to identify sub-topics within emails for comparison to our keyword-targeted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Libraries and Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.5-win-amd64.egg\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os, sys\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import email\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import sparse, hstack\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# run 'easy_install -U gensim' to install gensim and then re-launch jupyter notebook\n",
    "import gensim\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import constants\n",
    "import utils\n",
    "import vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (517401, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Message-ID: &lt;18782981.1075855378110.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Message-ID: &lt;15464986.1075855378456.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>Message-ID: &lt;24216240.1075855687451.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Message-ID: &lt;13505866.1075863688222.JavaMail.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Message-ID: &lt;30922949.1075863688243.JavaMail.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            message\n",
       "0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n",
       "1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n",
       "2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n",
       "3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n",
       "4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test import; runtime ~ 5 minutes for full set\n",
    "emails_df = pd.read_pickle(path + '/cleaned_emails.pkl')\n",
    "print(emails_df.shape)\n",
    "emails_df.head()\n",
    "\n",
    "# ENTER number of rows for training; NONE if using all\n",
    "num_rows = 50000\n",
    "\n",
    "if num_rows != None:\n",
    "    mini_df = emails_df.loc[range(0,num_rows),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Baseline: K-Means Clustering with Simple N-gram Features\n",
    "\n",
    "Using partition of early emails only to reduce dimensions in baseline bi-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams shape:  (50000, 168196)\n",
      "N-grams TF-IDF: (50000, 168196)\n"
     ]
    }
   ],
   "source": [
    "# consider longer n-grams\n",
    "transformer = TfidfTransformer()\n",
    "vectorize = CountVectorizer(ngram_range=(1, 2), max_df=.01, min_df=5)\n",
    "n_grams = vectorize.fit_transform(mini_df[\"email_str\"])\n",
    "print(\"N-grams shape: \", n_grams.shape)\n",
    "\n",
    "n_grams_idf = transformer.fit_transform(n_grams)\n",
    "print(\"N-grams TF-IDF:\", n_grams_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.01, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit k-means (n=4, tol=.01, max_iter=100 takes ~20 mins to train)\n",
    "kmeans = KMeans(n_clusters=5, tol=.01, max_iter=100)\n",
    "kmeans.fit(n_grams_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 clusters: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Positive labels:   [2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions on positively labeled examples\n",
    "base_preds = kmeans.predict(n_grams_idf)\n",
    "print(\"First 30 clusters:\", base_preds[:30])\n",
    "\n",
    "# GET IDS OF POSITIVE LABELS\n",
    "positive_labels = emails_df[\"suspicious_ind\"][emails_df[\"suspicious_ind\"]==1]\n",
    "\n",
    "print(\"Positive labels:  \", base_preds[positive_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 75864)\n"
     ]
    }
   ],
   "source": [
    "# evaluate closest examples in cluster 2 based on cosine similarity\n",
    "suspicious_ids = np.zeros(len(base_preds))\n",
    "for k in label_dict.keys():\n",
    "    suspicious_ids[k] = label_dict.get(k)\n",
    "\n",
    "# enter index of typical cluster\n",
    "key_cluster = 2\n",
    "\n",
    "cluster_ids = (base_preds==key_cluster).astype(int)\n",
    "\n",
    "features_np = sparse.csr_matrix.todense(feature_vects)\n",
    "\n",
    "labeled_feats = features_np[np.multiply(suspicious_ids, cluster_ids).astype(bool)]\n",
    "print(labeled_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([346, 27091, 373, 13966, 405])\n",
      "(5, 50000)\n",
      "[[21177 31738 39744 13961 11650 15532  7014   966   346  2759]\n",
      " [  985  2779  1893  2769   356   976  1879   373  2787   995]\n",
      " [  357   358  1892  2771   978   977  2820  1855   405  1027]\n",
      " [17248 10977 14371 11429 16116 13982 11606 13966  9998 17433]\n",
      " [20382 19225 29563 21933 27092 30259 24055 31124 27091 20569]]\n",
      "0.164362146998\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity\n",
    "print(label_dict.keys())\n",
    "cos_sims = cosine_similarity(labeled_feats, feature_vects)\n",
    "print(cos_sims.shape)\n",
    "closest = np.argsort(cos_sims, axis = 1)\n",
    "print(closest[:,-10:])\n",
    "print(cos_sims[1,985])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> jacques , the agreement looks fine . </s> <s> my only comment is that george and larry might object to the language that `` the bank that was requested to finance the construction of the project declined to make the loan based on the high costs of the construction of the project '' . </s> <s> <unk> , that bank lowered the loan amount based on lower estimates of <unk> which <unk> the amount of equity that would be required . </s> <s> did i loan them $ DGDGDGDGDGDGDG ? </s> <s> i thought it was less . </s> <s> regarding exhibit a , the assets include : the land , <unk> plans , engineering completed , appraisal , and <unk> study . </s> <s> most of these items are in a state of partial completion by the consultants . </s> <s> i have been speaking directly to the architect , engineer , and <unk> engineer . </s> <s> i am unclear on what is the best way to proceed with these consultants . </s> <s> the obligations should include the fees owed to the consultants above . </s> <s> do we need to list balances due or just list the work completed as an asset and give consideration of $ DGDGDGDG for the cash paid to the engineer and <unk> . </s> <s> phillip </s>\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nearest emails flagged:\n",
    "emails_df.loc[985, \"email_str\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 K-Means Clustering using Paragraph Vectors (doc2vec)\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['here', 'is', 'our', 'forecast', '<s>'], [0])\n"
     ]
    }
   ],
   "source": [
    "# create tagged documents for model training\n",
    "tagger = gensim.models.doc2vec.TaggedDocument\n",
    "tagged_docs = []\n",
    "for i, email in enumerate(emails_df[\"email_list\"]):\n",
    "    tagged_docs.append(tagger(email, [i]))\n",
    "\n",
    "print(tagged_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.5-win-amd64.egg\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "doc_model = gensim.models.doc2vec.Doc2Vec(documents=tagged_docs, vector_size = 1000, window = 5, alpha = .01, min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40709, 0.6937914490699768), (40678, 0.6881959438323975), (40849, 0.6868060231208801), (44116, 0.684868574142456), (7543, 0.6820092797279358), (44276, 0.680620014667511), (42515, 0.6795379519462585), (42274, 0.6784195899963379), (40589, 0.6751448512077332), (42558, 0.673206090927124)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_model.docvecs.most_similar(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec help:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "https://stackoverflow.com/questions/41709318/what-is-gensims-docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_feats = doc_model.docvecs.doctag_syn0\n",
    "print(doc_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_doc = KMeans(n_clusters=5, tol=.01, max_iter=100)\n",
    "kmeans_doc.fit(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
