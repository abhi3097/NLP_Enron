{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Deception in Enron Emails\n",
    "## 3. Modeling\n",
    "\n",
    "In this notebook, we attempt three methods of clustering and topic modeling, beginning with simple N-gram features, then reducing dimensionality for K-means using paragraph vectoring, or Doc2Vec. We also try variants of Latent Dirichlet Allocation (LDA) to identify sub-topics within emails for comparison to our keyword-targeted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Libraries and Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os, sys\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import email\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import sparse, hstack\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# run 'easy_install -U gensim' to install gensim and then re-launch jupyter notebook\n",
    "#import gensim\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import constants\n",
    "import utils\n",
    "import vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(517401, 3)\n"
     ]
    }
   ],
   "source": [
    "# test import; runtime ~ 5 minutes for full set\n",
    "path = '/home/cmiller11/NLP_Enron'\n",
    "emails_df = pd.read_pickle(path + '/cleaned_emails.pkl')\n",
    "print(emails_df.shape)\n",
    "emails_df.head()\n",
    "\n",
    "# ENTER number of rows for training; NONE if using all\n",
    "num_rows = 50000\n",
    "\n",
    "if num_rows != None:\n",
    "    mini_df = emails_df.loc[range(0,num_rows),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Baseline: K-Means Clustering with Simple N-gram Features\n",
    "\n",
    "Using partition of early emails only to reduce dimensions in baseline bi-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams shape:  (50000, 168196)\n",
      "N-grams TF-IDF: (50000, 168196)\n"
     ]
    }
   ],
   "source": [
    "# consider longer n-grams\n",
    "transformer = TfidfTransformer()\n",
    "vectorize = CountVectorizer(ngram_range=(1, 2), max_df=.01, min_df=5)\n",
    "n_grams = vectorize.fit_transform(mini_df[\"email_str\"])\n",
    "print(\"N-grams shape: \", n_grams.shape)\n",
    "\n",
    "n_grams_idf = transformer.fit_transform(n_grams)\n",
    "print(\"N-grams TF-IDF:\", n_grams_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.01, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit k-means (n=4, tol=.01, max_iter=100 takes ~20 mins to train)\n",
    "kmeans = KMeans(n_clusters=5, tol=.01, max_iter=100)\n",
    "kmeans.fit(n_grams_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 clusters: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Positive labels:   [2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions on positively labeled examples\n",
    "base_preds = kmeans.predict(n_grams_idf)\n",
    "print(\"First 30 clusters:\", base_preds[:30])\n",
    "\n",
    "# GET IDS OF POSITIVE LABELS\n",
    "positive_labels = emails_df[\"suspicious_ind\"][emails_df[\"suspicious_ind\"]==1]\n",
    "\n",
    "print(\"Positive labels:  \", base_preds[positive_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 75864)\n"
     ]
    }
   ],
   "source": [
    "# evaluate closest examples in cluster 2 based on cosine similarity\n",
    "suspicious_ids = np.zeros(len(base_preds))\n",
    "for k in label_dict.keys():\n",
    "    suspicious_ids[k] = label_dict.get(k)\n",
    "\n",
    "# enter index of typical cluster\n",
    "key_cluster = 2\n",
    "\n",
    "cluster_ids = (base_preds==key_cluster).astype(int)\n",
    "\n",
    "features_np = sparse.csr_matrix.todense(feature_vects)\n",
    "\n",
    "labeled_feats = features_np[np.multiply(suspicious_ids, cluster_ids).astype(bool)]\n",
    "print(labeled_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([346, 27091, 373, 13966, 405])\n",
      "(5, 50000)\n",
      "[[21177 31738 39744 13961 11650 15532  7014   966   346  2759]\n",
      " [  985  2779  1893  2769   356   976  1879   373  2787   995]\n",
      " [  357   358  1892  2771   978   977  2820  1855   405  1027]\n",
      " [17248 10977 14371 11429 16116 13982 11606 13966  9998 17433]\n",
      " [20382 19225 29563 21933 27092 30259 24055 31124 27091 20569]]\n",
      "0.164362146998\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity\n",
    "print(label_dict.keys())\n",
    "cos_sims = cosine_similarity(labeled_feats, feature_vects)\n",
    "print(cos_sims.shape)\n",
    "closest = np.argsort(cos_sims, axis = 1)\n",
    "print(closest[:,-10:])\n",
    "print(cos_sims[1,985])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> jacques , the agreement looks fine . </s> <s> my only comment is that george and larry might object to the language that `` the bank that was requested to finance the construction of the project declined to make the loan based on the high costs of the construction of the project '' . </s> <s> <unk> , that bank lowered the loan amount based on lower estimates of <unk> which <unk> the amount of equity that would be required . </s> <s> did i loan them $ DGDGDGDGDGDGDG ? </s> <s> i thought it was less . </s> <s> regarding exhibit a , the assets include : the land , <unk> plans , engineering completed , appraisal , and <unk> study . </s> <s> most of these items are in a state of partial completion by the consultants . </s> <s> i have been speaking directly to the architect , engineer , and <unk> engineer . </s> <s> i am unclear on what is the best way to proceed with these consultants . </s> <s> the obligations should include the fees owed to the consultants above . </s> <s> do we need to list balances due or just list the work completed as an asset and give consideration of $ DGDGDGDG for the cash paid to the engineer and <unk> . </s> <s> phillip </s>\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nearest emails flagged:\n",
    "emails_df.loc[985, \"email_str\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 K-Means Clustering using Paragraph Vectors (doc2vec)\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['here', 'is', 'our', 'forecast', '<s>'], [0])\n"
     ]
    }
   ],
   "source": [
    "# create tagged documents for model training\n",
    "tagger = gensim.models.doc2vec.TaggedDocument\n",
    "tagged_docs = []\n",
    "for i, email in enumerate(emails_df[\"email_list\"]):\n",
    "    tagged_docs.append(tagger(email, [i]))\n",
    "\n",
    "print(tagged_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Colby\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.5-win-amd64.egg\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "doc_model = gensim.models.doc2vec.Doc2Vec(documents=tagged_docs, vector_size = 1000, window = 5, alpha = .01, min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(40709, 0.6937914490699768), (40678, 0.6881959438323975), (40849, 0.6868060231208801), (44116, 0.684868574142456), (7543, 0.6820092797279358), (44276, 0.680620014667511), (42515, 0.6795379519462585), (42274, 0.6784195899963379), (40589, 0.6751448512077332), (42558, 0.673206090927124)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_model.docvecs.most_similar(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec help:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "https://stackoverflow.com/questions/41709318/what-is-gensims-docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_feats = doc_model.docvecs.doctag_syn0\n",
    "print(doc_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_doc = KMeans(n_clusters=5, tol=.01, max_iter=100)\n",
    "kmeans_doc.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topic Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "(10000, 3000)\n"
     ]
    }
   ],
   "source": [
    "#Implement LDA for topics of each document\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#hyperparameters\n",
    "random.seed(24)\n",
    "#n_samples = 2000\n",
    "n_features = 3000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "rand_num = 10000\n",
    "\n",
    "rand_ids = random.sample(range(emails_df.shape[0]), rand_num)\n",
    "email_samples = emails_df.loc[rand_ids,]\n",
    "#training_emails = all_emails.loc[all_ids,]\n",
    "\n",
    "train_content = list(email_samples['email_str'])\n",
    "#dev_content = list(email_samples['content_str'])\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "#                                   max_features=n_features,\n",
    "#                                   stop_words='english')\n",
    "\n",
    "#tfidf = tfidf_vectorizer.fit_transform(all_content)\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.4, min_df=10,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                ngram_range = (1,1))\n",
    "#t0 = time()\n",
    "#tf = tf_vectorizer.fit_transform(data_samples)\n",
    "#print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#print()\n",
    "\n",
    "tf_train = tf_vectorizer.fit_transform(train_content)\n",
    "tf_vocab = tf_vectorizer.vocabulary_\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print(tf_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_features=3000...\n",
      "done in 1418.911s.\n",
      "with 349 iterations\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: thanks know meeting need let time cc like kay attached issues questions group information work pm forward review et discuss\n",
      "Topic #1: image font td 09 br size width com tr dgdgdg href height face border table arial fantasy img align sportsline\n",
      "Topic #2: power energy california said state market dgdgdg price prices iso electricity ferc new utilities final customers commission gas electric rate\n",
      "Topic #3: com dgdgdg mail sent message original pm cc fax net dgdgdgdgdg monday aol mailto john fw chris thanks october sara\n",
      "Topic #4: com message error vince intended mail recipient kaminski database information confidential use email dgdgdg doc corp delete distribution occurred sender\n",
      "Topic #5: 20 company business gas new market million services energy 01 management president companies group year dgdgdg markets risk houston natural\n",
      "Topic #6: dgdgdg com pm information click time contact id email new access scheduled sat center web mail available data outages london\n",
      "Topic #7: ect hou ees cc corp na pm enron_development enronxgate lon mark communications john david richard chris thanks jones jeff scott\n",
      "Topic #8: agreement 3d dgdgdgdgdgdg deal contract trading ena dgdgdgdgdg credit gas master legal day attached isda deals transaction dgdgdg trade new\n",
      "Topic #9: just like know time day good way com week ll going great think new yahoo make did people want night\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "#hyperparameters\n",
    "lda_iterations = 1000\n",
    "    \n",
    "    \n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_features=%d...\"\n",
    "      % (n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=lda_iterations,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50., n_jobs = -1,\n",
    "                                random_state=0, evaluate_every = 50, verbose = False)\n",
    "t0 = time()\n",
    "lda.fit(tf_train)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"with {} iterations\".format(lda.n_iter_))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after raptor there are 229 indices\n",
      "after ljm there are 544 indices\n",
      "after fraud there are 892 indices\n",
      "after manipulation there are 1326 indices\n",
      "after condor there are 1356 indices\n",
      "after trutta there are 1368 indices\n",
      "after swap there are 4900 indices\n",
      "after differential there are 5187 indices\n",
      "after ferc there are 10836 indices\n",
      "after merlin there are 10935 indices\n",
      "after whitewing there are 10979 indices\n",
      "after jedi there are 11094 indices\n",
      "after vehicle there are 12206 indices\n",
      "there are 484399 emails non relevant emails to sample from after removing 33002 relevant emails\n",
      "There are 10000 irrelevant docs\n",
      "the train email df will be 22206\n",
      "Extracting tf features for LDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 22206\n",
      "INFO:guidedlda:vocab_size: 22830\n",
      "INFO:guidedlda:n_words: 5675974\n",
      "INFO:guidedlda:n_topics: 10\n",
      "INFO:guidedlda:n_iter: 700\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of CountVectorizer:  (22206, 22830)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmiller11/anaconda3/lib/python3.6/site-packages/guidedlda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:guidedlda:<0> log likelihood: -62165857\n",
      "INFO:guidedlda:<50> log likelihood: -47299041\n",
      "INFO:guidedlda:<100> log likelihood: -46943247\n",
      "INFO:guidedlda:<150> log likelihood: -46820467\n",
      "INFO:guidedlda:<200> log likelihood: -46746635\n",
      "INFO:guidedlda:<250> log likelihood: -46716167\n",
      "INFO:guidedlda:<300> log likelihood: -46703662\n",
      "INFO:guidedlda:<350> log likelihood: -46680900\n",
      "INFO:guidedlda:<400> log likelihood: -46670569\n",
      "INFO:guidedlda:<450> log likelihood: -46664131\n",
      "INFO:guidedlda:<500> log likelihood: -46658764\n",
      "INFO:guidedlda:<550> log likelihood: -46651523\n",
      "INFO:guidedlda:<600> log likelihood: -46646283\n",
      "INFO:guidedlda:<650> log likelihood: -46645347\n",
      "INFO:guidedlda:<699> log likelihood: -46645476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 22830)\n",
      "Topic 0: ees ferc ect hou na et transmission rto corp market pm staff commission james order comments meeting need steffes\n",
      "Topic 1: energy gas power natural new pipeline electric capacity paso prices el market com plant california dgdgdgdgdg price 09 services\n",
      "Topic 2: 20 market information new rate risk time business provide 01 use contract issues trading service order need customers project\n",
      "Topic 3: 20 3d power state california energy said electricity market prices utilities price font davis federal new utility ferc commission\n",
      "Topic 4: com mail message gov doc org net intended ca recipient mailto aol bpa bracepatt received email confidential energy john\n",
      "Topic 5: company said million new financial stock business dow jones trading billion energy news credit year investors 20 shares companies\n",
      "Topic 6: com california market iso mail ferc power energy price electricity 20 prices state order cap wholesale jeff utilities generators\n",
      "Topic 7: ect hou corp pm thanks enron_development agreement swap sara mark know sent need credit attached shackleton let deal isda\n",
      "Topic 8: pm time com message know original sent like sat just scheduled contact let week london houston thanks good think\n",
      "Topic 9: image com 09 day new click houston tx rates travel online information dgdgdgdgdg net way ca hotel hilton specials\n",
      "finished glda in 470.5567021369934\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import guidedlda\n",
    "import math\n",
    "\n",
    "t0 = time()\n",
    "random.seed(24)\n",
    "pct_keyword_docs = 0.35\n",
    "sample_keyword_doc_idx = set()\n",
    "all_keyword_doc_idx = set()\n",
    "phrases = [\"raptor\",\"ljm\",\"fraud\",\"manipulation\", \"condor\",\"trutta\",\"swap\",\\\n",
    "               \"differential\",\"ferc\", \"merlin\", \"whitewing\", \"jedi\", \"vehicle\"]\n",
    "for phrase in phrases:\n",
    "    query = emails_df[emails_df['email_str'].str.contains(phrase, case=False)]\n",
    "    matching_indices = query.index.tolist()\n",
    "    all_keyword_doc_idx = all_keyword_doc_idx.union(set(matching_indices))\n",
    "    if len(matching_indices) == 0:\n",
    "        print(phrase + \" did not yield any results\")\n",
    "        continue\n",
    "    num_keyword_docs = math.ceil(len(matching_indices)*pct_keyword_docs)\n",
    "    rand_ids = random.sample(matching_indices, num_keyword_docs)\n",
    "    sample_keyword_doc_idx = sample_keyword_doc_idx.union(set(rand_ids))\n",
    "    print(\"after {} there are {} indices\".format(phrase,len(sample_keyword_doc_idx)))     \n",
    "\n",
    "guided_emails = emails_df.loc[list(sample_keyword_doc_idx)]\n",
    "\n",
    "#hyperparameters\n",
    "random.seed(24)\n",
    "n_features = None\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "rand_num = 10000\n",
    "seed_confidence = 0.99\n",
    "max_iters = 700\n",
    "\n",
    "all_idx = set()\n",
    "for i in emails_df.index:\n",
    "    all_idx.add(i)\n",
    "available_emails_idx = all_idx.difference(all_keyword_doc_idx)\n",
    "print(\"there are {} emails non relevant emails to sample from after removing {} relevant emails\"\\\n",
    ".format(len(available_emails_idx),len(all_keyword_doc_idx)))\n",
    "rand_ids = random.sample(list(available_emails_idx), rand_num)\n",
    "train_idx = list(sample_keyword_doc_idx.union(set(rand_ids)))     \n",
    "email_samples = emails_df.loc[train_idx,]\n",
    "\n",
    "print(\"There are {} irrelevant docs\".format(rand_num))\n",
    "print(\"the train email df will be {}\".format(email_samples.shape[0]))\n",
    "\n",
    "train_content = list(email_samples['email_str'])\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.40, min_df=10,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                ngram_range = (1,1))\n",
    "\n",
    "tf_train = tf_vectorizer.fit_transform(train_content)\n",
    "tf_vocab = tf_vectorizer.vocabulary_\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print(\"Output of CountVectorizer: \",tf_train.shape)\n",
    "             \n",
    "seed_topic_list = [phrases]\n",
    "seed_topics = {}\n",
    "for t_id, keywords in enumerate(seed_topic_list):\n",
    "    for word in keywords:\n",
    "        seed_topics[tf_feature_names.index(word)] = t_id\n",
    "    \n",
    "glda = guidedlda.GuidedLDA(n_topics = n_components, n_iter = max_iters, random_state = 7, refresh = 50)\n",
    "glda.fit(tf_train, seed_topics = seed_topics, seed_confidence = seed_confidence)\n",
    "topic_word = glda.topic_word_\n",
    "print(topic_word.shape)\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "print(\"finished glda in {}\".format(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349\n",
      "[0.00691522 0.02868753 0.00651866]\n",
      "[ 272 1125  250]\n"
     ]
    }
   ],
   "source": [
    "print(tf_feature_names.index('raptor'))\n",
    "print(topic_word[:,349])\n",
    "print(glda.nzw_[:,349])\n",
    "#email_samples['email_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try the fit method\n",
    "dev_vectorizer = CountVectorizer(vocabulary = tf_vocab)\n",
    "tf_dev = dev_vectorizer.fit_transform(dev_content)\n",
    "doc_topic_distr = lda.transform(tf_dev)\n",
    "print(doc_topic_distr[1])\n",
    "print(dev_content[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
