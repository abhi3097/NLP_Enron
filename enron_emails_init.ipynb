{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os, sys\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import email\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "# Helper libraries\n",
    "import constants\n",
    "import utils\n",
    "import vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Email Bodies\n",
    "Data source and exploration code: [Kaggle](https://www.kaggle.com/zichen/explore-enron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5, 2)\n",
      "Message-ID: <15464986.1075855378456.JavaMail.evans@thyme>\n",
      "Date: Fri, 4 May 2001 13:51:00 -0700 (PDT)\n",
      "From: phillip.allen@enron.com\n",
      "To: john.lavorato@enron.com\n",
      "Subject: Re:\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Phillip K Allen\n",
      "X-To: John J Lavorato <John J Lavorato/ENRON@enronXgate@ENRON>\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail\n",
      "X-Origin: Allen-P\n",
      "X-FileName: pallen (Non-Privileged).pst\n",
      "\n",
      "Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\n",
      "\n",
      "As far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \n",
      "\n",
      "My suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load csv dataset - download from Kaggle (linked above, ~.5gb)\n",
    "\n",
    "# replace with local path\n",
    "path = 'C:/Users/Colby/Documents/Berkeley/266_NLP/final_project/data'\n",
    "\n",
    "emails_df = pd.read_csv(path + '/emails.csv', nrows=5)\n",
    "#emails_df = pd.read_csv(path + '/emails.csv')\n",
    "\n",
    "print(\"Shape:\", emails_df.shape)\n",
    "emails_df.head()\n",
    "print(emails_df['message'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1.</td>\n",
       "      <td>Here is our forecast\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10.</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p/_sent_mail/100.</td>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p/_sent_mail/1000.</td>\n",
       "      <td>Randy,\\n\\n Can you send me a schedule of the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p/_sent_mail/1001.</td>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file                                            content\n",
       "0     allen-p/_sent_mail/1.                          Here is our forecast\\n\\n \n",
       "1    allen-p/_sent_mail/10.  Traveling to have a business meeting takes the...\n",
       "2   allen-p/_sent_mail/100.                     test successful.  way to go!!!\n",
       "3  allen-p/_sent_mail/1000.  Randy,\\n\\n Can you send me a schedule of the s...\n",
       "4  allen-p/_sent_mail/1001.                Let's shoot for Tuesday at 11:45.  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isolate email body\n",
    "\n",
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append( part.get_payload() )\n",
    "    return ''.join(parts)\n",
    "\n",
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, emails_df['message']))\n",
    "emails_df.drop('message', axis=1, inplace=True)\n",
    "\n",
    "# Parse content from emails\n",
    "emails_df['content'] = list(map(get_text_from_email, messages))\n",
    "\n",
    "del messages\n",
    "\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and canonicalize each email\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "all_words = []\n",
    "for i, body in enumerate(emails_df[\"content\"]):\n",
    "    tokens = tokenizer.tokenize(body)\n",
    "    canon = utils.canonicalize_words(tokens)\n",
    "    emails_df[\"content\"][i] = canon\n",
    "    all_words += canon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 100\n",
      "Vocabulary dict: {'discussions': 33, 'then': 34, 'play': 35, 'forecast': 36, 'opinions': 37, 'suggest': 52, 'meetings.': 39, \"'s\": 40, 'speaks': 41, 'time': 42, 'meeting': 43, 'in': 31, 'groups': 44, 'our': 46, ')': 58, ',': 16, 'meetings': 10, 'business': 8, 'on': 17, 'discussion': 49, 'would': 11, 'flying': 50, 'i': 12, 'especially': 38, 'where': 53, 'a': 4, 'much': 54, 'changes': 55, 'quiet': 57, 'across': 59, 'might': 60, 'that': 61, 'desired': 62, '<s>': 0, 'test': 63, 'held': 64, 'waiting': 65, 'somewhere': 66, 'trip.': 67, 'for': 9, 'others': 69, 'out': 70, 'better': 71, 'trip': 18, 'fun': 72, 'think': 73, 'whether': 74, 'presentation.': 75, 'presenter': 76, 'let': 77, 'often': 78, 'salary': 79, 'round': 80, 'dgdg:dgdg': 81, \"ski's.\": 83, 'jet': 84, 'group.': 56, 'working': 85, 'get': 45, 'my': 86, 'go': 22, '.': 23, 'are': 88, 'even': 24, 'ski': 89, 'different': 90, 'scheduling': 91, 'far': 92, 'the': 3, 'is': 7, 'about': 94, 'turn.': 95, 'takes': 25, 'made.': 96, 'rent': 98, 'patti': 99, '</s>': 1, 'be': 14, 'as': 27, 'can': 97, 'here': 28, 'to': 6, 'holding': 87, 'try': 29, 'have': 30, 'too': 26, 'productive': 82, '<unk>': 2, 'and': 5, 'send': 93, 'or': 51, 'tuesday': 47, 'level': 68, 'any': 19, 'of': 15, 'what': 20, '!': 13, 'you': 32, 'some': 48, 'if': 21}\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "#vocab = vocabulary.Vocabulary(all_words, size=None)  # size=None means unlimited\n",
    "V = 100\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(emails_df[\"content\"])), size=V)\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "print(\"Vocabulary dict:\", vocab.word_to_id)\n",
    "#vocab_ids = vocab.words_to_ids(all_words)\n",
    "#vocab_counter = collections.Counter(all_words)\n",
    "#vocab_counter\n",
    "\n",
    "# need to properly handle punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'successful.', 'way', 'to', 'go', '!', '!', '!']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_df[\"content\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_array = utils.preprocess_sentences(emails_df[\"content\"], vocab, use_eos=True, emit_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<s>', 'here', 'is', 'our', 'forecast', '</s>', '<s>', '<unk>',\n",
       "       'to', 'have', 'a', 'business', 'meeting', 'takes', 'the', 'fun',\n",
       "       'out', 'of', 'the', 'trip.', 'especially', 'if', 'you', 'have',\n",
       "       'to', '<unk>', 'a', 'presentation.', 'i', 'would'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_array[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".....\n",
    "## (from Assignment 2)\n",
    "\n",
    "# Prelude: working with raw text data\n",
    "\n",
    "You're probably accustomed to a typical machine learning set-up, where each example consists of a (fixed) set of features $x^{(i)}$ and an associated target $y^{(i)}$. _(Note: when there's any ambiguity, we'll use superscripts like $x^{(i)}$ to refer to the $i^{th}$ example in the dataset, and subscripts like $x_j$ to refer to the $j^{th}$ feature or vector element.)_\n",
    "\n",
    "If we have $d$ features, we might have $x \\in \\mathbb{R}^{d}$, and in a $k$-way classification problem, we have $y \\in \\{0,1,\\ldots,k - 1\\}$. Some of our features might be indicators, $x_j \\in \\{0,1\\}$, and some subset of our features might be sparse - in that most of them are zero, most of the time.\n",
    "\n",
    "Of course, text data doesn't start in this form. Instead, what we get is a sentence, and we'll need to apply a few steps to convert it into a representation suitable for machine learning. As an example, let's consider sentiment analysis, where our labels $y \\in \\{0,1\\}$ represent negative or positive sentiment. A training example might be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text, y = (\"I love W266!\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to do much to $y$, but to get $x$ ready for a classifier we need to:\n",
    "1. Tokenize the text into individual words (tokens)\n",
    "2. Canonicalize the tokens\n",
    "3. Convert the tokens to a sequence of integer IDs\n",
    "4. (optional) Convert the IDs to a feature vector\n",
    "\n",
    "The integer IDs are often referred to as _word ids_ or _token ids_. These might be familiar from live session, when we converted the word \"the\" into a column index in an embedding table. (In another sense, indices are just a representation of a one-hot vector.)\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "This can be as simple as calling `string.split()` - good enough for English and many European languages - but we could also do something more sophisticated here. For now, let's just use the Penn Treebank tokenizer, which uses a set of regular expressions to do things like separate punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'W266', '!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "x_tokens = tokenizer.tokenize(x_text)\n",
    "x_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization\n",
    "\n",
    "Depending on the application, we might want to do some pre-processing to remove spurious variation in the text. For example, we might want to lowercase words to avoid storing separate features for \"I\" and \"i\", and we might want to replace numbers with a special token rather than keep track of every possible value.\n",
    "\n",
    "We've implemented the above transformation in `utils.canonicalize_word`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Example: '$1.50' -> '{:s}'\".format(utils.canonicalize_word(\"$1.50\")))\n",
    "print(\"Example: 'FooBar' -> '{:s}'\".format(utils.canonicalize_word(\"FooBar\")))\n",
    "x_tokens_canonical = utils.canonicalize_words(x_tokens)\n",
    "x_tokens_canonical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to IDs\n",
    "\n",
    "While there are a few ML models that operate directly on strings - you'll implement two of them in Assignment 5 - in most cases (and *always* for neural networks) you'll need to convert the tokens to integer IDs that can index into a feature vector. To do this, we'll need to keep track of a **vocabulary**, which in its simplest form is just a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_dict = {\"!\": 0, \"i\": 1, \"love\": 2, \"w266\": 3}\n",
    "x_ids = [vocab_dict[token] for token in x_tokens_canonical]\n",
    "x_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in most cases the corresponding feature vector has a fixed size, and occasionally you'll encounter words that the model has never seen before. Typically, we'll replace such words with a special token, `<unk>`.\n",
    "\n",
    "In the example below, note that the third token gets ID 0, corresponding to `<unk>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppose we don't know about the word 'w266'\n",
    "vocab_dict = {w:i for i, w in enumerate([\"<unk>\", \"!\", \"i\", \"love\"])}\n",
    "print(vocab_dict)\n",
    "x_ids = [vocab_dict.get(token, vocab_dict['<unk>']) for token in x_tokens_canonical]\n",
    "x_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of this class, we'll use a `Vocabulary` object to manage this conversion. We've implemented one in the `w266_common.vocabulary` module, and most NLP packages will have something that works similarly.\n",
    "\n",
    "Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vocabulary.Vocabulary(x_tokens_canonical, size=None)  # size=None means unlimited\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "print(\"Vocabulary dict: \", vocab.word_to_id)\n",
    "x_ids = vocab.words_to_ids(x_tokens_canonical)\n",
    "print(\"x_ids =\", x_ids)\n",
    "x_tokens_recovered = vocab.ids_to_words(x_ids)\n",
    "x_tokens_recovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the additional `<s>` and `</s>` tokens - these are used to represent the beginning and end of sentences. We'll make use of these in Assignment 3 and Assignment 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting IDs to a feature vector\n",
    "\n",
    "Many NLP models are designed to work directly with a sequence of word ids. However, for many standard machine learning models such as Naive Bayes, SVMs, or Logistic Regression, we need to convert this sequence to a fixed-length vector $x \\in \\mathbb{R}^d$. In the general case, we can define a set of feature extractors $f_i$ for $i = 0,1,\\ldots,d-1$, such that $x_i = f_i([\\mathtt{ids}])$.\n",
    "\n",
    "The simplest way to do this is a bag-of-words model, in which we let the number of features be the size of our vocabulary ($d = |V|$), and we let each feature be a count of the number of times word $i$ appears in the sequence:\n",
    "\n",
    "$$ f_i([\\mathtt{ids}]) = \\sum_{j = 0}^{n} \\mathbf{1}[w_j = i] $$\n",
    "\n",
    "We can do this in a very simple way by using the `collections.Counter` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Example, with words as keys:\", collections.Counter(x_tokens_canonical))\n",
    "x_fdict = collections.Counter(x_ids)\n",
    "x_fdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common data format to use in machine learning applications is to transform this dictionary-like object that maps keys to values into a feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = vocab.size  # one feature for each word\n",
    "x_vector = [x_fdict.get(i, 0) for i in range(num_features)]\n",
    "x_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one has multiple examples, these are represented as multiple rows of these vectors stacked on top of one another (similar to the batching you did in assignment 1).  If $|V|$ is large and the text short, it is likely most of the elements of such a matrix are zero.  A memory optimization can be made by using a [sparse vector](https://docs.scipy.org/doc/scipy/reference/sparse.html) representation. (Or for more than one example, a sparse matrix.) You may have worked with these before, as they are the preferred input format for many `scikit-learn` ML routines.\n",
    "\n",
    "The sparse matrix constructor requires three parallel lists: the row indices, the column indices, and the corresponding values.  Note that we have just a single row (we have only one example), so all the row indices are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix\n",
    "from scipy.sparse import csr_matrix \n",
    "\n",
    "row_indices = []\n",
    "col_indices = []\n",
    "values = []\n",
    "\n",
    "# Construct three parallel lists as described above to satisfy the sparse matrix constructor.\n",
    "for wordid, count in x_fdict.items():\n",
    "    row_indices.append(0)       # only a single example, so row 0\n",
    "    col_indices.append(wordid)  # column is word id\n",
    "    values.append(count)        # value is count\n",
    "x_sparse = csr_matrix((values, (row_indices, col_indices)),\n",
    "                      shape=[1, vocab.size])\n",
    "print(\"Non-zero values:\")\n",
    "print(x_sparse)\n",
    "x_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a helper function, `utils.id_lists_to_sparse_bow` that can handle this conversion over a whole dataset, and in most cases we'll handle this conversion for you in the starter code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
